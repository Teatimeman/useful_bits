{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive the gradients of cross entropy loss w.r.t. inputs\n",
    "\n",
    "2017-04-14 jkang  \n",
    "Python 3.5\n",
    "\n",
    "Ref:\n",
    "- http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/\n",
    "\n",
    "\n",
    "## Softmax function and derivatives\n",
    "Softmax function maps from input $\\hat{x}$ to output probability $\\hat{s}$ ( $\\hat{x}$ and $\\hat{s}$ is $\\mathbb{R}^{C\\times1}$, C is number of classes )\n",
    "$$ \\hat{s} = \\mathbf{softmax}(\\hat{x}) $$\n",
    "\n",
    "which can be written for each class as:\n",
    "$$\\begin{split}\n",
    "s_i &= \\mathbf{softmax}_i(x_i) \\\\\n",
    "&= \\frac{e^{x_i}}{\\sum_{c=1}^C e^{x_c}} \\\\\n",
    "\\end{split}$$\n",
    "\n",
    "We are interested in how much output $s$ changes by input $x$, but given, $s_i$ all $x$ values affect the changes of $s_i$. So to generalize the relationship between $s_i$ and $x$, we need to separate where $i=j$ ($s_i$, $x_i$) and $i\\ne{j}$ ($s_i$, $x_j$).\n",
    "\n",
    "Before calculating the derivative of softmax function, it's better to know what **quotient rule** is. This rule applies to calculating the derivative of the quotient of two function as:\n",
    "$$ \\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2} $$\n",
    "\n",
    "Here is the calculation of the **derivatives of softmax function**.\n",
    "$$\\begin{split} \n",
    "When\\ i = j,\\ & \\frac{\\partial s_i}{\\partial x_i} = \\frac{\\partial \\frac{e^{x_i}}{\\sum_{c=1}^C e^{x_c}}}{\\partial x_i} = \\frac{1}{\\partial x_i} \\cdot \\frac{x_i e^{x_i}\\sum_{c=1}^C e^{x_c} - e^{x_i}\\cdot x_i e^{x_i}}{(\\sum_{c=1}^C e^{x_c})^2} \\\\\n",
    "&= \\frac{e^{x_i} \\sum_{c=1}^C e^{x_c} - \\cdot e^{x_i} e^{x_i}}{(\\sum_{c=1}^C e^{x_c})^2} = \\frac{e^{x_i}}{\\sum_{c=1}^C e^{x_c}} \\cdot \\frac{\\sum_{c=1}^C e^{x_c} - e^{x_i}}{\\sum_{c=1}^C e^{x_c}} \\\\\n",
    "&= \\frac{e^{x_i}}{\\sum_{c=1}^C e^{x_c}} \\left( 1 - \\frac{e^{x_i}}{\\sum_{c=1}^C e^{x_c}} \\right) \\\\\n",
    "&= s_i (1 - s_i)\n",
    "\\end{split}$$\n",
    "\n",
    "$$\\begin{split}\n",
    "When\\ i \\ne{j},\\ & \\frac{\\partial s_i}{\\partial x_j} = \\frac{\\partial \\frac{e^{x_i}}{\\sum_{c=1}^C e^{x_c}}}{\\partial x_j} = \\frac{1}{\\partial x_j} \\cdot \\frac{0 \\cdot \\sum_{c=1}^C e^{x_c} - e^{x_i}\\cdot x_j e^{x_j}}{(\\sum_{c=1}^C e^{x_c})^2} \\\\\n",
    "&= -\\frac{e^{x_i}e^{x_j}}{(\\sum_{c=1}^C e^{x_c})^2} = -\\frac{e^{x_i}}{\\sum_{c=1}^C e^{x_c}} \\cdot \\frac{e^{x_j}}{\\sum_{c=1}^C e^{x_c}} = -s_i s_j\n",
    "\\end{split}$$\n",
    "\n",
    "## Cross-entropy loss function and derivatives\n",
    "$$\\begin{split}\n",
    "&\\quad CE(y,\\ \\hat{y}) = -\\sum_{i}y_ilog(\\hat{y}_i) \\\\ \\\\\n",
    "&\\textrm{where, y is (n by 1: one-hot coding)} \\\\\n",
    "&\\hat{y}\n",
    "\\end{split}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
