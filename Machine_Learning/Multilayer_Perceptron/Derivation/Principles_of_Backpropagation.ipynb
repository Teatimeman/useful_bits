{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Backpropagation\n",
    "2017-04-28 jkang  \n",
    "\n",
    "* This notes summarized notations and fundamentals for backpropagation in Neural Network for simpler understanding and easier implementation in code\n",
    "* All notations are vectorized for simplification\n",
    "\n",
    "Ref:  \n",
    "- http://neuralnetworksanddeeplearning.com/\n",
    "- https://cs224d.stanford.edu/\n",
    "- http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/\n",
    "\n",
    "---\n",
    "\n",
    "## Notations\n",
    "\n",
    "> ### Data\n",
    "> * $X$ is an input matrix; size = $(n\\_inputs)\\ \\times \\ (n\\_features)$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Inputs are stacked row-wise in $X$  \n",
    "> * $Y$ is an output matrix; size = $(n\\_outputs)\\ \\times \\ (n\\_classes)$  \n",
    "\n",
    "> ### Network  \n",
    "\n",
    "> * $W^k$ is a weight matrix which maps $(k-1)$th layer to $k$th layer\n",
    "> * $b^k$ is a bias vector at $k$th layer\n",
    "\n",
    "> ### Processes\n",
    "\n",
    "> * $C$ is a cost function. The choices of cost function can be Cross-entropy, MSE, etc.\n",
    "> * $\\sigma(X)$ is a sigmoid function which maps X into $\\sigma(X)$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; c.f. $\\sigma'(X)$ is derivative of $\\sigma(X)$\n",
    "> * $z^l$ is the weighted sum of inputs to $l$th layer (before applied to the activation function); size = $(n\\_inputs)\\ \\times \\ (n\\_hidden\\_units)$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; c.f. $z^L$ is the weighted sum at the final layer\n",
    "> * $a^l$ is a transformed version of $z$ by the activation function; size( $z^l$ ) = size( $a^l$ )\n",
    "> * $\\delta^L$ is the final output error at $z^L$; i.e. $\\frac{\\partial(C)}{\\partial(z^L)}$; size( $\\delta^L$ ) = size( $Y$ )\n",
    "> * $\\delta^l$ is the $l$th error at $z^l$; size( $\\delta^l$ ) = size( $a^l$ )  \n",
    ">> Why are they called '**error**'? Short answer: this 'error' tells us how sensitive each layer is to the cost. This sensitivity is important because it helps us to know how much we can change weights and biases to reduce the cost. Bottom line is the 'error' appears in deriving $\\frac{\\partial C}{\\partial w}$. So, it would be good to know how much this error happens and make it explicit for later calculations. See [Nielson](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "## Goal\n",
    "\n",
    "> Understand how much network parameters ( $W$ and $b$ ) affect $C$, and calculate the proper amount for parameter update (i.e. derivatives of parameters)  \n",
    "> Calculate: $$\\frac{\\partial C}{\\partial W}\\ and\\ \\frac{\\partial C}{\\partial b}$$  \n",
    "> Update $k$th weights and biases:\n",
    "> $$W^k = W^k - \\eta\\frac{\\partial C}{\\partial W^k}$$\n",
    "> $$b^k = b^k - \\eta\\frac{\\partial C}{\\partial b^k}$$\n",
    "\n",
    "## Fundamentals for Backpropagation\n",
    "\n",
    "> ### BP rules (=Back-Propagation)  \n",
    "\n",
    "> ### <p style=\"color:blue;font-weight:bold\">BP 1: How much the little change in $z^L$ (in the last layer) affect $C$?</p>\n",
    "> $$\\delta^L = \\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial z^L} \\odot \\sigma '(z^L)$$\n",
    "> ### <p style=\"color:blue;font-weight:bold\">BP 2: What's the relationship between $\\delta^l$ and $\\delta^{l+1}$</p>\n",
    "> $$\\delta^l = ((W^{l+1})^T \\odot \\delta^{l+1}) $$\n",
    "> ### <p style=\"color:blue;font-weight:bold\">BP 3: How much does the bias $b^l$ affect $C$?</p>\n",
    "> $$\\frac{\\partial C}{\\partial b^l} = \\delta^l$$\n",
    "> ### <p style=\"color:blue;font-weight:bold\">BP 4: How much does the weight $W^l$ affect $C$?</p> \n",
    "> $$\\frac{\\partial C}{\\partial W^{l+1}} = a^l \\odot \\delta^{l+1}$$\n",
    "\n",
    "## Example\n",
    "\n",
    "* One hidden layer Neural Network architecture from [cs224d](https://cs224d.stanford.edu/) (1st assignment):\n",
    "    ![1-layer_NN](https://github.com/jaekookang/useful_bits/blob/master/Machine_Learning/Multilayer_Perceptron/Derivation/ipynb_data/1-layer_NN.png?raw=true)\n",
    "<br><br>\n",
    "\n",
    "* Dimensions\n",
    "\n",
    "> $X \\in \\mathbb{R}^{n \\times m}$, **n**: number of examples, **m**: number of input dimensions  \n",
    "> $Y \\in \\mathbb{R}^{n \\times c}$, **c**: number of output units (labels)  \n",
    ">   - NB. Here $X$ and $Y$ are used as an explicit matrix notation rather than a vector $x$ or $y$ (or $\\hat{y}$).  \n",
    "\n",
    "> $H \\in \\mathbb{R}^{n \\times h}$, **h**: number of hidden units  \n",
    "> $W^1 \\in \\mathbb{R}^{m \\times h}$  \n",
    "> $W^2 \\in \\mathbb{R}^{h \\times c}$  \n",
    "> $b^1 \\in \\mathbb{R}^{1 \\times h}$, they are copied column-wise to make $n \\times h$ matrix for calculation  \n",
    "> $b^2 \\in \\mathbb{R}^{1 \\times c}$, they are copied column-wise to make $n \\times c$ matrix for calculation    \n",
    "\n",
    "* Activation function at the hidden layer:   \n",
    "$$\\begin{split}\n",
    "\\sigma(X) &= sigmoid(XW^1 + b^1) \\\\ \n",
    "&= \\sigma(XW^1 + b^1)\n",
    "\\end{split}$$\n",
    "<br><br>\n",
    "\n",
    "* Output function at the outpur layer:\n",
    "$$\\begin{split} \n",
    "\\hat{Y} &= softmax(HW^2 + b^2) \\\\\n",
    "CE(Y, \\hat{Y}) &= \\sum_{i=1}^n\\sum_{j=1}^c Y_{ij} \\cdot log(\\hat{Y_{ij}})\n",
    "\\end{split}$$\n",
    "\n",
    "### Feedforward process\n",
    "> $$\\begin{split}\n",
    "H &= \\sigma(XW^1 + b^1) \\\\\n",
    "Z &= HW^2 + b^2 \\\\\n",
    "\\hat{Y} &= softmax(Z) \\\\\n",
    "&= f(Z)\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "### Backward propagation  \n",
    "> #### Get output error  (BP1)\n",
    "> $$\\begin{split} \n",
    "\\delta^2 &= \\frac{\\partial CE(Y, \\hat{Y})}{\\partial Z} \\\\\n",
    "&= \\frac{\\partial CE(Y, \\hat{Y})}{\\partial f(Z)} \\cdot \\frac{\\partial f(Z)}{\\partial Z} \\\\\n",
    "&= \\hat{Y} - Y \\\\\n",
    "\\end{split}$$  \n",
    "> For more, look [Peter's notes](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)\n",
    "\n",
    "> #### Backpropagate the error (BP2, BP3, BP4)\n",
    "> * Since our goal is to calculate \n",
    "> $$\\begin{split}\n",
    "\\frac{\\partial C}{\\partial W^1} &= X \\cdot \\delta^1 \\quad (BP4) \\\\\n",
    "\\frac{\\partial C}{\\partial b^1} &= \\delta^1 \\quad (BP3) \\\\\n",
    "\\frac{\\partial C}{\\partial W^2} &= H \\cdot \\delta^2 \\quad (BP4) \\\\\n",
    "\\frac{\\partial C}{\\partial b^2} &= \\delta^2 \\quad (BP3) \\\\\n",
    "\\end{split}$$  \n",
    "> we need to calculate $\\delta^1$, which can be calculated using **BP2**  \n",
    "> $$\\delta^1 = ((W^2)^T) \\odot \\delta^2$$  \n",
    "\n",
    "> #### Update parameters\n",
    "> * $\\eta$ is a learning rate  \n",
    "> $$\\begin{split}\n",
    "W^{1\\ new} &= W^1 - \\eta \\frac{\\partial C}{\\partial W^1} \\\\\n",
    "b^{1\\ new} &= b^1 - \\eta \\frac{\\partial C}{\\partial b^1} \\\\\n",
    "W^{2\\ new} &= W^2 - \\eta \\frac{\\partial C}{\\partial W^2} \\\\\n",
    "b^{2\\ new} &= b^2 - \\eta \\frac{\\partial C}{\\partial b^2} \\\\\n",
    "\\end{split}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
