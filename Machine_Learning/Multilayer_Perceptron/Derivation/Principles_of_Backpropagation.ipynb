{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Backpropagation\n",
    "2017-04-28 jkang  \n",
    "\n",
    "This notes summarized notations and fundamentals for backpropagation in Neural Network for simpler understanding and easier implementation in code\n",
    "\n",
    "Ref:  \n",
    "- http://neuralnetworksanddeeplearning.com/\n",
    "- https://cs224d.stanford.edu/\n",
    "\n",
    "---\n",
    "\n",
    "## Notations\n",
    "\n",
    "> ### Data\n",
    "> * $X$ is an input matrix; size = $(n\\_inputs)\\ \\times \\ (n\\_features)$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;Inputs are stacked row-wise in $X$  \n",
    "> * $Y$ is an output matrix; size = $(n\\_outputs)\\ \\times \\ (n\\_classes)$  \n",
    "\n",
    "> ### Network  \n",
    "\n",
    "> * $W^k$ is a weight matrix which maps $(k-1)$th layer to $k$th layer\n",
    "> * $b^k$ is a bias vector at $k$th layer\n",
    "\n",
    "> ### Processes\n",
    "\n",
    "> * $C$ is a cost function. The choices of cost function can be Cross-entropy, MSE, etc.\n",
    "> * $\\sigma(X)$ is a sigmoid function which maps X into $\\sigma(X)$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; c.f. $\\sigma'(X)$ is derivative of $\\sigma(X)$\n",
    "> * $z^l$ is the weighted sum of inputs to $l$th layer (before applied to the activation function); size = $(n\\_inputs)\\ \\times \\ (n\\_hidden\\_units)$  \n",
    "> &nbsp;&nbsp;&nbsp;&nbsp; c.f. $z^L$ is the weighted sum at the final layer\n",
    "> * $a^l$ is a transformed version of $z$ by the activation function; size( $z^l$ ) = size( $a^l$ )\n",
    "> * $\\delta^L$ is the final output error at $z^L$; i.e. $\\frac{\\partial(C)}{\\partial(z^L)}$; size( $\\delta^L$ ) = size( $Y$ )\n",
    "> * $\\delta^l$ is the $l$th error at $z^l$; size( $\\delta^l$ ) = size( $a^l$ )  \n",
    ">> Why are they called '**error**'? Short answer: this 'error' tells us how sensitive each layer is to the cost. This senstivitive is important because it helps us to know how much we can change weights and biases to reduce the cost. Bottom line is the 'error' appears in the derivative of $\\frac{\\partial C}{\\partial w}$ calculation. So, it would be good to know this error and make it explicit for later calculations. See [Nielson](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "\n",
    "## Goal\n",
    "\n",
    "> Understand how much network parameters ( $W$ and $b$ ) affect $C$, and calculate the proper amount for parameter update (i.e. derivatives of parameters)  \n",
    "> Calculate: $$\\frac{\\partial C}{\\partial W}\\ and\\ \\frac{\\partial C}{\\partial b}$$  \n",
    "> Update $k$th weights and biases:\n",
    "> $$W^k = W^k - \\eta\\frac{\\partial C}{\\partial W^k}$$\n",
    "> $$b^k = b^k - \\eta\\frac{\\partial C}{\\partial b^k}$$\n",
    "\n",
    "## Fundamentals for Backpropagation\n",
    "\n",
    "> ### BP rules (=Back-Propagation)  \n",
    "\n",
    "> ### <p style=\"color:blue;font-weight:bold\">BP 1: How much the little change in $z^L$ affect $C$?</p>\n",
    "> $$\\delta^L = \\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial z^L} \\odot \\sigma '(z^L)$$\n",
    "> ### <p style=\"color:blue;font-weight:bold\">BP 2: What's the relationship between $\\delta^l$ and $\\delta^{l+1}$</p>\n",
    "> $$\\delta^l = ((W^{l+1})^T \\odot \\delta^{l+1}) $$\n",
    "> ### <p style=\"color:blue;font-weight:bold\">BP 3: How much does the bias $b$ affect $C$?\n",
    "> $$$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
