{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Machine Translation (NMT)\n",
    "\n",
    "2017-04-15 jkang  \n",
    "python3.5  \n",
    "\n",
    "### This tutorial covers following concepts for NMT (based on [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf)):\n",
    "* <font size='4'>What is NMT?</font>\n",
    "    * Definition of NMT\n",
    "    * Loss function  \n",
    "<br>\n",
    "* <font size='4'>What is **'Attention-based'** NMT?</font>\n",
    "    * Definition of attention\n",
    "    * Hard attention? Soft attention?\n",
    "    * How to model attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Machine Translation (NMT)?\n",
    "* <font size='4'>Definition</font>  \n",
    "    * According to [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf), NMT is \"a <font color='blue'>Neural Network</font> that directly models the __**conditional probability**__ $p(y|x)$ of translating a source sentence ($x_1, x_2, ... x_n $) to a target sentence ($y_1, y_2, ... y_n $)\".  \n",
    "    * This means, NMT learns the probability of <font size='4' color='green'>target language</font> given <font size='4' color='blue'> source language</font>.\n",
    "    * Here is the **conditional probability**:\n",
    "    \n",
    "    $\\begin{align*}\n",
    "\\log{p(y|x)} &= \\sum_{j=1}^m {\\log{p(y_j|y_{<j}, \\mathbf{s})}} \\\\\n",
    "&= \\log {\\prod_{j=1}^{m} p(y_j|y_{<j}, \\mathbf{s})} \\qquad ...What\\ does\\ it\\ mean??\\ \\\\\n",
    "\\end{align*}$\n",
    "    * For example, I want to say, <font size='4' color='green'>\"I want to eat an apple\"</font> to my Korean friend and One possible translation will be <font size='4' color='blue'>\"나는 사과 한개를 먹고 싶다\"</font>  \n",
    "    * First the model learns the source language. \n",
    "    * The source is <font size='4' color='green'>\"I want to eat an apple\"</font> and learning the source language is called **<font color='green'>Encoding</font>**\n",
    "    * Next, the model predicts the target language, <font size='4' color='blue'>\"나는 사과 한개를 먹고 싶다\"</font> (**<font color='blue'>Decoding</font>**)\n",
    "    * This time, the model spits out one word at a time based on what you have learned ( $\\mathbf{s}$ ) from the source language\n",
    "    * This $\\mathbf{s}$ is a source representation and also called \"**attention**\" vector\n",
    "\n",
    "\n",
    "* <font size='4'>Loss function</font>  \n",
    "    * So, NMT has two parts: <font color='green'>encoder</font> and <font color='blue'>decoder</font>\n",
    "    * Output translation comes only in <font color='blue'>decoder</font> part\n",
    "    * Let's look at the figure to make it clear\n",
    "    * From [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf):  \n",
    "    <img src=\"/Users/jaegukang/GoogleDrive/GitHub/useful_bits_tmp/pic.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is '**Attention-based**' NMT?\n",
    "(coming soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
