{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Machine Translation (NMT)\n",
    "\n",
    "2017-04-15 jkang  \n",
    "python3.5  \n",
    "\n",
    "### This tutorial covers following concepts for NMT (based on [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf)):\n",
    "* What is NMT?\n",
    "    * Definition of NMT\n",
    "    * Loss function  \n",
    "<br>\n",
    "* What is **'Attention-based'** NMT?\n",
    "    * Definition of attention\n",
    "    * Global attention? Local attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Machine Translation (NMT)?\n",
    "* <p style=\"font-size:20px\">Definition</p>\n",
    "    * According to [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf), NMT is \"a Neural Network that directly models the __**conditional probability**__ $p(y|x)$ of translating a source sentence ($x_1, x_2, ... x_n $) to a target sentence ($y_1, y_2, ... y_n $)\".  \n",
    "    * This means, NMT learns the probability of target language given  source language.\n",
    "    * Here is the **conditional probability**:\n",
    "    \n",
    "    > $\\begin{align*}\n",
    "\\log{p(y|x)} &= \\sum_{j=1}^m {\\log{p(y_j\\ |\\ y_{<j}, \\mathbf{s})}} \\\\\n",
    "&= \\log {\\prod_{j=1}^{m} p(y_j\\ |\\ y_{<j}, \\mathbf{s})} \\qquad ...What\\hspace{2mm} does\\hspace{2mm} it\\hspace{2mm} mean??\\ \\\\\n",
    "\\end{align*}$\n",
    "    * Let's break down the equation above\n",
    "    * The left-side log probability, $\\log{p(y|x)}$, simply means finding the best sequence of translation. \n",
    "    > For example, think about English to Korean translation.  \n",
    "    > if $x$ is \"I want an apple\" (English), $y$ will be \"나는 사과 한개를 원해\" (Korean).  \n",
    "    > $\\log{p(y|x)}$ will assign the highest log probability to \"나는 사과 한개를 원해\",  \n",
    "    > not \"나는 _감자_ 한개를 원해\".  \n",
    "    * The right side of the sum of log probabilities, $\\sum_{j=1}^m {\\log{p(y_j\\ |\\ y_{<j}, \\mathbf{s})}}$, includes the actual process of **decoding** the source sentence \"I want an apple\".\n",
    "    > It will be sum of:  \n",
    "    > $\\log{p(y_1|\\ \\mathbf{s})}\\ +$  \n",
    "    > $\\log{p(y_2|\\ y_{1},\\ \\mathbf{s})}\\ +$  \n",
    "    > $\\log{p(y_3|\\ \\{y_1,\\ y_2\\},\\ \\mathbf{s})}\\ +$  \n",
    "    > $\\log{p(y_3|\\ \\{y_1,\\ y_2,\\ y_3\\},\\ \\mathbf{s})}\\ +\\ ...$  \n",
    "    * As you can see, $\\mathbf{s}$ is added constantly to predict/decode the next targe word.\n",
    "    * This $\\mathbf{s}$ is called \"source representation\" or \"thought vector\", which is referred to as \"attention mechanism\"\n",
    "    > $\\mathbf{s}\\ =\\ ''Attention''$\n",
    "\n",
    "* <p style=\"font-size:20px\">Loss function</p>\n",
    "    * So, NMT has two parts: encoder and decoder\n",
    "    * Output translation comes only in decoder part\n",
    "    * Let's look at the figure to make it clear\n",
    "    * From [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf):  \n",
    "    <img src=\"ipynb_data/luong_etal_2015.png\" width=\"400\" height=\"300\" />\n",
    "    * The loss function is simply defined as minimizing sum of negative log probabilities\n",
    "    > $\\begin{align*} \\\\\n",
    "    J_t &= \\sum_{(x,y)\\in\\mathbb{D}}-\\log{p(y\\ |\\ x)} \\\\\n",
    "    & *\\mathbb{D}\\hspace{2mm} is\\hspace{2mm} training\\hspace{2mm} corpus\n",
    "    \\end{align*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is '**Attention-based**' NMT?\n",
    "* <p style=\"font-size:20px\">Definition of attention</p>\n",
    "    * Attention in NMT is a fixed-length vector including information about input\n",
    "    * Attention mechanism works to help predict/decode outputs as in the image:\n",
    "    <img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-17-at-10.39.06-AM.png\" width=\"400\" height=\"300\" />\n",
    "    <p style=\"text-align:center\">*Image from <a href=\"http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\">WildML</a></p>\n",
    "    <p style=\"text-align:center\">$h_3$ serves as attention vector</p>\n",
    "    <img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.16.08-PM.png\" width=\"200\" height=\"100\" />\n",
    "    <p style=\"text-align:center\">*Image from <a href=\"http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/\">WildML</a></p>\n",
    "    <p style=\"text-align:center\">The $\\alpha$ vector is fed into decoding the next output</p>\n",
    "<br>    \n",
    "    * In [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf), they used global attention and local attention mechanism\n",
    "    > **Global attention** includes feeding the attention vector calculated from all inputs everytime when decoding the next output  \n",
    "    > **Local attention**, on the other hand, indicates providing the attention vector calculated from the certain windowed portion of input to decoding the next output\n",
    "* <p style=\"font-size:20px\"> Global attention model:</p>\n",
    "    * Luong et al defined global attention as __soft attention__ which is differentiable\n",
    "    <img src=\"ipynb_data/luong_global.png\" width=\"300\" height=\"200\" />\n",
    "* <p style=\"font-size:20px\"> Local attention model:</p>\n",
    "    * Luong et al defined local attention as __hard attention__ which is non-differentiable\n",
    "    <img src=\"ipynb_data/luong_local.png\" width=\"300\" height=\"200\" />\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
