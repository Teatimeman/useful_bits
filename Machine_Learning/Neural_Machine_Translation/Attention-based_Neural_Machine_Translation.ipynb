{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Machine Translation (NMT)\n",
    "\n",
    "2017-04-15 jkang  \n",
    "python3.5  \n",
    "\n",
    "### This tutorial covers following concepts for NMT (based on [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf)):\n",
    "* What is NMT?\n",
    "    * Definition of NMT\n",
    "    * Loss function  \n",
    "<br>\n",
    "* What is **'Attention-based'** NMT?\n",
    "    * Definition of attention\n",
    "    * Hard attention? Soft attention?\n",
    "    * How to model attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Machine Translation (NMT)?\n",
    "* Definition  \n",
    "    * According to [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf), NMT is \"a Neural Network that directly models the __**conditional probability**__ $p(y|x)$ of translating a source sentence ($x_1, x_2, ... x_n $) to a target sentence ($y_1, y_2, ... y_n $)\".  \n",
    "    * This means, NMT learns the probability of target language given  source language.\n",
    "    * Here is the **conditional probability**:\n",
    "    \n",
    "    $\\begin{align*}\n",
    "\\log{p(y|x)} &= \\sum_{j=1}^m {\\log{p(y_j|y_{<j}, \\mathbf{s})}} \\\\\n",
    "&= \\log {\\prod_{j=1}^{m} p(y_j|y_{<j}, \\mathbf{s})} \\qquad ...What\\ does\\ it\\ mean??\\ \\\\\n",
    "\\end{align*}$\n",
    "    * For example, I want to say, \"I want to eat an apple\" to my Korean friend and One possible translation will be \"나는 사과 한개를 먹고 싶다\"\n",
    "    * First the model learns the source language. \n",
    "    * The source is \"I want to eat an apple\" and learning the source language is called **Encoding**\n",
    "    * Next, the model predicts the target language, \"나는 사과 한개를 먹고 싶다\" (**Decoding**)\n",
    "    * This time, the model spits out one word at a time based on what you have learned ( $\\mathbf{s}$ ) from the source language\n",
    "    * This $\\mathbf{s}$ is a source representation and also called \"**attention**\" vector\n",
    "\n",
    "\n",
    "* Loss function\n",
    "    * So, NMT has two parts: encoder and decoder\n",
    "    * Output translation comes only in decoder part\n",
    "    * Let's look at the figure to make it clear\n",
    "    * From [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf):  \n",
    "    ![](ipynb_data/luong_etal_2015.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is '**Attention-based**' NMT?\n",
    "(coming soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
