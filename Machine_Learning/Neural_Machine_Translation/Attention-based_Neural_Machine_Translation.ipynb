{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Neural Machine Translation (NMT)\n",
    "\n",
    "2017-04-15 jkang  \n",
    "python3.5  \n",
    "\n",
    "### This tutorial covers following concepts for NMT (based on [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf)):\n",
    "* What is NMT?\n",
    "    * Definition of NMT\n",
    "    * Loss function  \n",
    "<br>\n",
    "* What is **'Attention-based'** NMT?\n",
    "    * Definition of attention\n",
    "    * Hard attention? Soft attention?\n",
    "    * How to model attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Neural Machine Translation (NMT)?\n",
    "* Definition  \n",
    "    * According to [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf), NMT is \"a Neural Network that directly models the __**conditional probability**__ $p(y|x)$ of translating a source sentence ($x_1, x_2, ... x_n $) to a target sentence ($y_1, y_2, ... y_n $)\".  \n",
    "    * This means, NMT learns the probability of target language given  source language.\n",
    "    * Here is the **conditional probability**:\n",
    "    \n",
    "    > $\\begin{align*}\n",
    "\\log{p(y|x)} &= \\sum_{j=1}^m {\\log{p(y_j|y_{<j}, \\mathbf{s})}} \\\\\n",
    "&= \\log {\\prod_{j=1}^{m} p(y_j|y_{<j}, \\mathbf{s})} \\qquad ...What\\ does\\ it\\ mean??\\ \\\\\n",
    "\\end{align*}$\n",
    "    * Let's break down the equation above\n",
    "    * The left-side log probability, $\\log{p(y|x)}$, simply means finding the best sequence of translation. \n",
    "    > For example, think about English to Korean translation.  \n",
    "    > if $x$ is \"I want an apple\" (English), $y$ will be \"나는 사과 한개를 원해\" (Korean).  \n",
    "    > $\\log{p(y|x)}$ will assign the highest log probability to \"나는 사과 한개를 원해\", not \"나는 감자 한개를 원해\".  \n",
    "    * The right side of the sum of log probabilities, $\\sum_{j=1}^m {\\log{p(y_j|y_{<j}, \\mathbf{s})}}$, includes the actual process of **decoding** the source sentence \"I want an apple\".\n",
    "\n",
    "* Loss function\n",
    "    * So, NMT has two parts: encoder and decoder\n",
    "    * Output translation comes only in decoder part\n",
    "    * Let's look at the figure to make it clear\n",
    "    * From [Luong et al., 2015](https://arxiv.org/pdf/1508.04025.pdf):  \n",
    "    <img src=\"ipynb_data/luong_etal_2015.png\" width=\"300\" height=\"200\"/>\n",
    "    <font color='blue'/>\n",
    "    * Hey there\n",
    "    <font color='black'/>\n",
    "    * I'm back\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is '**Attention-based**' NMT?\n",
    "(coming soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
