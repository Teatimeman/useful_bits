{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Encoder-Decoder for Machine Translation\n",
    "\n",
    "2017-04-30 jkang\n",
    "\n",
    "### This tutorial summarizes statistical concepts for RNN Encoder-Decoder for Machine Translation System\n",
    "\n",
    "ref:\n",
    "- [Cho et al. (2014)](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "---\n",
    "\n",
    "## What is RNN?\n",
    "- A recurrent neural network (RNN) is an architecture where variable-length input sequence ($\\mathbf{x} = (x_1, x_2, ... , x_T)$) is fed into the network to generate the hidden state and optionally output $\\mathbf{y}$, according to [Cho et al. (2014)](https://arxiv.org/abs/1406.1078).\n",
    "- Hidden state in RNN is updated as in\n",
    "> $$\\mathbf{h}_{t} = f(\\mathbf{h}_{t-1},\\ x_t)$$  \n",
    "> - $\\mathbf{h}_{t}$ is hidden state at time $t$\n",
    "> - $f$ is a non-linear activate function (can be simple as sigmoid function or complex as LSTM)\n",
    "- RNN predicts next output based on previous sequence\n",
    "- RNN learns a probability distribution\n",
    "> $$p(x_{x,j} = 1 | x_{t-1}, ... ,x_1) = \\frac{exp(\\mathbf{w}_j \\mathbf{h}_t)}{\\sum_{j'=1}^K exp(\\mathbf{w}_{j'} \\mathbf{h}_t)}$$  \n",
    "> - All possible labels are from $j=1, ... K$  \n",
    "> - $\\mathbf{w}_j$ are the rows of a weight matrix $\\mathbf{W}$\n",
    "> - \"How probable it is to predict $x_j$ at time $t$ given the previous sequence\"  \n",
    "- To generalize the above equation into probability of the sequence $\\mathbf{x}$\n",
    "$$p(x) = \\prod_{t=1}^T p(x_t | x_{t-1}, ... , x_1)$$\n",
    "\n",
    "\n",
    "\n",
    "## RNN Encoder-Decoder\n",
    "- RNN Encoder-Decoder is \"a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence\"\n",
    "> $$e.g. \\quad p(y_1, ... y_{T'} | x_1, ... ,x_T)$$  \n",
    "> - Output length $T'$ and inptut length $T$ are not the same\n",
    "- Diagram for RNN Encoder-Decoder from [Cho et al. (2014)](https://arxiv.org/abs/1406.1078)  \n",
    "![rnn_encoder_decoder](https://github.com/jaekookang/useful_bits/blob/master/Machine_Learning/Neural_Machine_Translation/ipynb_data/rnn_encoder_decoder.png?raw=true)\n",
    "> - Hidden state of the decoder at time $t$ is computed as\n",
    "> $$\\mathbf{h}_t = f(\\mathbf{h}_{t-1}, y_{t-1}, \\mathbf{c})$$  \n",
    "> $$P(y_t | y_{t-1}, y_{t-2}, ... , y_1, \\mathbf{c}) = g(\\mathbf{h}_t, y_{t-1}, \\mathbf{c})$$\n",
    "> - $\\mathbf{c}$ is a fixed-length encoded vector of the input sequence\n",
    "> - $g$ is used to decode the probability of the output\n",
    "- To summarize, RNN Encoder-Decoder is a conditional probability mapping from inputs to outputs (or \"jointly trained model to maxtmize the conditional log-likelihood\")\n",
    "> $$\\arg\\max_\\theta \\frac{1}{N}\\sum_{n=1}^N log p_{\\theta}(\\mathbf{y}_n | \\mathbf{x}_n)$$  \n",
    "> - $N$ is a number of training examples\n",
    "> - ($\\mathbf{x}_n$, $\\mathbf{y}_n$) is an input sequence and output sequence pair\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
